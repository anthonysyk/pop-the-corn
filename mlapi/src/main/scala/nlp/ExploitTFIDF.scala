package nlp

import nlp.TFIDF.Article
import org.apache.spark.SparkConf
import org.apache.spark.mllib.linalg
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{SaveMode, SparkSession}


object ExploitTFIDF {

//  val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("ExploitTFIDF")
//  val ss: SparkSession = SparkSession.builder().config(conf).getOrCreate()


  def sparkConfMap(isProduction: Boolean = true): Map[String, String] = {

    Map(
      "app.name" → "test",
      "rdd.compress" → "true",
      "ui.enables" → "true",
      "serializer" → "org.apache.spark.serializer.KryoSerializer"
    ) ++ (if (isProduction)
      Map(
        "master" → "yarn-client",
        "driver.memory" → "12g",
        "executor.memory" → "16g",
        "dynamicAllocation.enabled" → "true",
        "yarn.executor.memoryOverhead" → "4000",
        "default.parallelism" → "128",
        "suffle.manager" → "hash",
        "suffle.service.enabled" → "true",
        "kryoserializer.buffer.max" → "512m",
        "dynamicAllocation.maxExecutors" → "50",
        "sql.parquet.mergeSchema" → "false"
      )
    else
      Map(
        "master" → "local[*]"
      ))

  }

  def getSparkConf(isProduction: Boolean = true): SparkConf = {
    sparkConfMap(isProduction).foldLeft(new SparkConf())({ case (conf, (k, v)) ⇒ conf.set(s"spark.$k", v) })
  }

  val conf: SparkConf = new SparkConf()
  SparkSession.builder().config(conf.setMaster("local[*]")).getOrCreate()

  val ss: SparkSession = SparkSession.builder().config(conf).getOrCreate()


  case class MovieMatrix(indexId: Long, movieId: Int, scores: Map[String, Double], inputVector: Seq[Double], vector: linalg.Vector, similarities: Option[MatrixEntry] = None)

  case class MovieSimilarity(idIndex: Long, idMovie: Int, similarity: Seq[MatrixEntry])

  def main(args: Array[String]): Unit = {

    val path = getClass.getResource("/tfidf-movies").getPath
    val prodPath = "/home/usr_trafgar/test/resources/tfidf-movies"

    import ss.implicits._
    val articleRDD = ss.read.parquet(prodPath).as[Article].rdd.sortBy(_.id).persist()

    println(articleRDD.count)

    val dictionnary: Seq[String] = articleRDD.flatMap { article =>
      article.words
    }.distinct().collect().toSeq

    val scoresByMovie: RDD[MovieMatrix] = articleRDD.zipWithIndex.map {
      case (movie, index) => MovieMatrix(index, movie.id, movie.score.toMap, Nil, Vectors.dense(Array.empty[Double]))
    }

    val preCosineSimilarityMovies: RDD[MovieMatrix] = scoresByMovie.map { movieMatrix =>
      val inputVector = dictionnary.map { word =>
        movieMatrix.scores.getOrElse(word, 0.0)
      }
      val vector = Vectors.dense(inputVector.toArray)
      movieMatrix.copy(inputVector = inputVector, vector = vector)
    }

    val indexedRows: RDD[IndexedRow] = preCosineSimilarityMovies.map { movieMatrix =>
      IndexedRow(movieMatrix.indexId, movieMatrix.vector)
    }.persist

    val irm: IndexedRowMatrix = new IndexedRowMatrix(indexedRows)

    val resultMatrix: RDD[MatrixEntry] = irm.toCoordinateMatrix().transpose().toRowMatrix().columnSimilarities().entries

    resultMatrix.toDF.coalesce(1).write.mode(SaveMode.Overwrite).parquet("/home/usr_trafgar/test/similarity-matrix")

    val pathLocal = "mlapi/src/main/resources/similarity-matrix"

    ss.close()
  }

}
